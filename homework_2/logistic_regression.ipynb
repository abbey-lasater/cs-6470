{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5646fa9",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a logistic regression model from scratch for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e82ac0",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f9b3602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, roc_curve, auc, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3371f",
   "metadata": {},
   "source": [
    "Pulling Data Set In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "46850e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading wine data in from sklearn datasets\n",
    "wine = load_wine()\n",
    "\n",
    "# converting data into pandas df, only including feature columns\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc0b13",
   "metadata": {},
   "source": [
    "Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1b0d5016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating target variable\n",
    "df['target'] = wine.target\n",
    "\n",
    "# filtering to only include classes 0 and 1 for binary classification\n",
    "df_binary = df[df['target'].isin([0, 1])]\n",
    "\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "66b74ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting columns names in list\n",
    "columns=wine.feature_names\n",
    "\n",
    "# feature values\n",
    "X = df_binary[columns].values\n",
    "\n",
    "# target value\n",
    "y = df_binary[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "90375be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling our features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ea863c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiaing list to store performance metrics\n",
    "performance_metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38656506",
   "metadata": {},
   "source": [
    "Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b0a691eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def cost_function(self, y_true, y_pred, eps=1e-15):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090703bd",
   "metadata": {},
   "source": [
    "log regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6350e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionManual(Model):\n",
    "    def __init__(self, lr, epochs, fit_intercept=True):\n",
    "        # Initialize coefficients (weights for features) as None\n",
    "        self.coef_ = None\n",
    "        # Initialize intercept (bias term) to 0\n",
    "        self.intercept_ = 0.0\n",
    "        # Whether to include an intercept in predictions\n",
    "        self.fit_intercept = fit_intercept\n",
    "        # Learning rate for gradient descent\n",
    "        self.lr = lr\n",
    "        # Number of iterations for training\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid function to map any real value to (0, 1) if y > .5, 1 else 0\n",
    "        # z is linear combination of features and weights\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #vectorize\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        # Number of samples and features\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize coefficients (weights) to zeros\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "        self.intercept_ = 0.0\n",
    "\n",
    "        # Gradient descent for specified number of epochs\n",
    "        for _ in range(self.epochs):\n",
    "            # Linear model: weighted sum of inputs plus intercept\n",
    "            # z = X * weights + intercept\n",
    "            linear_model = np.dot(X, self.coef_) + self.intercept_\n",
    "            # Apply sigmoid to convert linear scores to predicted probabilities\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients for weights and intercept -- 1/m * X.T * (y_pred - y)\n",
    "            # greadient wrt to weights\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            # gradient wrt to intercept\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and intercept using gradients (opposite dir of gradient) and learning rate\n",
    "            self.coef_ -= self.lr * dw\n",
    "            self.intercept_ -= self.lr * db\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        # predict probabilities using learned coefficients and intercept, returns 0<val<1\n",
    "        linear_model = np.dot(X, self.coef_) + self.intercept_\n",
    "        return self.sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X, threshold=.5):\n",
    "        # probabilities to get predicted classes 0 or 1 based on threshold\n",
    "        # if prob > threshold, class 1 else class 0\n",
    "        # default threshold is .5\n",
    "        y_predicted_probs = self.predict_proba(X)\n",
    "        return np.where(y_predicted_probs >= threshold, 1, 0)\n",
    "    \n",
    "    def cost_function(self, y_true, y_pred, eps=1e-15):\n",
    "        n = len(y_true)\n",
    "        total_loss = 0.0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            # Clamp predicted probability to avoid log(0)\n",
    "            yp = min(max(yp, eps), 1 - eps)\n",
    "\n",
    "            # Correct log loss formula\n",
    "            total_loss += yt * math.log(yp) + (1 - yt) * math.log(1 - yp)\n",
    "\n",
    "        return -total_loss / n\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a044d",
   "metadata": {},
   "source": [
    "applying logistric regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "010896e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegressionManual(\n",
    "    lr = .05, \n",
    "    epochs = 1000, \n",
    "    fit_intercept=True\n",
    ")\n",
    "\n",
    "#starting timer\n",
    "start = time.time()\n",
    "\n",
    "#fitting model\n",
    "log_reg.fit(X_scaled, y)\n",
    "\n",
    "#ending timer\n",
    "end = time.time()\n",
    "\n",
    "#predicting target variable\n",
    "y_pred_gd = log_reg.predict(X_scaled)\n",
    "\n",
    "y_pred_probabilites = log_reg.predict_proba(X_scaled)\n",
    "\n",
    "#appending perf metrics for model intno perf metric list\n",
    "performance_metrics.append(\n",
    "    {\n",
    "        \"Model\": \"Logistic Regression\",\n",
    "        \"Intercept\": log_reg.intercept_,\n",
    "        \"Coefficients\": log_reg.coef_,\n",
    "        \"Fit Time\": end - start,\n",
    "        \"Log Loss\": log_reg.cost_function(y, y_pred_probabilites)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b936d",
   "metadata": {},
   "source": [
    "Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6d866531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn logistic regression \n",
    "clf = LogisticRegression(fit_intercept=True, random_state=0, max_iter=1000)\n",
    "\n",
    "# starting timer \n",
    "start = time.time()\n",
    "\n",
    "# fitting model (use training set if you have one)\n",
    "clf.fit(X_scaled, y)\n",
    "\n",
    "# ending timer after fit is complete\n",
    "end = time.time()\n",
    "\n",
    "# predicting target var (on test set)\n",
    "y_pred_clf = clf.predict(X_scaled)\n",
    "\n",
    "# probabilities of each class\n",
    "y_pred_prob_clf = clf.predict_proba(X_scaled)\n",
    "\n",
    "# appending perf metrics\n",
    "performance_metrics.append(\n",
    "    {\n",
    "        \"Model\": \"Scikit-learn\",\n",
    "        \"Intercept\": clf.intercept_.tolist(),\n",
    "        \"Coefficients\": clf.coef_.flatten().tolist(),\n",
    "        \"Log Loss\": log_loss(y, y_pred_prob_clf),\n",
    "        \"Fit Time\": end - start,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b1613280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Model': 'Logistic Regression', 'Intercept': np.float64(0.23321491706700076), 'Coefficients': array([-1.67277596, -0.52356314, -0.94515328,  1.24233698, -0.28758118,\n",
      "       -0.10632093, -0.35759491,  0.18441816,  0.20693603, -0.99012793,\n",
      "        0.14825953, -0.59843979, -1.84743389]), 'Fit Time': 0.016946792602539062, 'Log Loss': np.float64(0.031524299828198994)}, {'Model': 'Scikit-learn', 'Intercept': [0.22595648396817414], 'Coefficients': [-1.5404136220653013, -0.49434902209952236, -0.9703864243666552, 1.2410124970878535, -0.23761342576419145, -0.03573489385890136, -0.32915200014943036, 0.1743875553449435, 0.18672440464621057, -0.7999574680304715, 0.15093769285273284, -0.6275202021200499, -1.8145746167337014], 'Log Loss': 0.03422152101737459, 'Fit Time': 0.013071775436401367}]\n"
     ]
    }
   ],
   "source": [
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2ce46289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model              Intercept  \\\n",
      "0  Logistic Regression               0.233215   \n",
      "1         Scikit-learn  [0.22595648396817414]   \n",
      "\n",
      "                                        Coefficients  Fit Time  Log Loss  \n",
      "0  [-1.6727759642951585, -0.5235631440656101, -0....  0.016947  0.031524  \n",
      "1  [-1.5404136220653013, -0.49434902209952236, -0...  0.013072  0.034222  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Coefficients</th>\n",
       "      <th>Fit Time</th>\n",
       "      <th>Log Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.233215</td>\n",
       "      <td>[-1.6727759642951585, -0.5235631440656101, -0....</td>\n",
       "      <td>0.016947</td>\n",
       "      <td>0.031524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scikit-learn</td>\n",
       "      <td>[0.22595648396817414]</td>\n",
       "      <td>[-1.5404136220653013, -0.49434902209952236, -0...</td>\n",
       "      <td>0.013072</td>\n",
       "      <td>0.034222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model              Intercept  \\\n",
       "0  Logistic Regression               0.233215   \n",
       "1         Scikit-learn  [0.22595648396817414]   \n",
       "\n",
       "                                        Coefficients  Fit Time  Log Loss  \n",
       "0  [-1.6727759642951585, -0.5235631440656101, -0....  0.016947  0.031524  \n",
       "1  [-1.5404136220653013, -0.49434902209952236, -0...  0.013072  0.034222  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# convert list of dicts into DataFrame\n",
    "perf_table = pd.DataFrame(performance_metrics)\n",
    "\n",
    "# show table\n",
    "print(perf_table)\n",
    "\n",
    "# if you want a nicer display in Jupyter\n",
    "from IPython.display import display\n",
    "display(perf_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b02d96",
   "metadata": {},
   "source": [
    "Analyzing and Comparing Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5c34e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confusion matrix, Accuracy,  Precision, Recall, and ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "89cddfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_manual = confusion_matrix(y, y_pred_gd)\n",
    "cm_sklearn = confusion_matrix(y, y_pred_clf)\n",
    "\n",
    "# Accuracy\n",
    "acc_manual = accuracy_score(y, y_pred_gd)\n",
    "acc_sklearn = accuracy_score(y, y_pred_clf)\n",
    "\n",
    "# Precision\n",
    "prec_manual = precision_score(y, y_pred_gd)\n",
    "prec_sklearn = precision_score(y, y_pred_clf)\n",
    "\n",
    "# Recall\n",
    "recall_manual = recall_score(y, y_pred_gd)\n",
    "recall_sklearn = recall_score(y, y_pred_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2bb9e151",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (130, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ROC curve values\u001b[39;00m\n\u001b[1;32m      2\u001b[0m fpr_manual, tpr_manual, _ \u001b[38;5;241m=\u001b[39m roc_curve(y, y_pred_probabilites)\n\u001b[0;32m----> 3\u001b[0m fpr_sklearn, tpr_sklearn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_prob_clf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# AUC (area under curve)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m roc_auc_manual \u001b[38;5;241m=\u001b[39m auc(fpr_manual, tpr_manual)\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/sklearn/metrics/_ranking.py:1150\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1047\u001b[0m     {\n\u001b[1;32m   1048\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1057\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m ):\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1150\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/sklearn/metrics/_ranking.py:822\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    820\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    821\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n\u001b[0;32m--> 822\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m assert_all_finite(y_true)\n\u001b[1;32m    824\u001b[0m assert_all_finite(y_score)\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/sklearn/utils/validation.py:1485\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn, device)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1473\u001b[0m             (\n\u001b[1;32m   1474\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1479\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1480\u001b[0m         )\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(\n\u001b[1;32m   1482\u001b[0m         xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp, device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m   1483\u001b[0m     )\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1487\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (130, 2) instead."
     ]
    }
   ],
   "source": [
    "# ROC curve values\n",
    "fpr_manual, tpr_manual, _ = roc_curve(y, y_pred_probabilites)\n",
    "fpr_sklearn, tpr_sklearn, _ = roc_curve(y, y_pred_prob_clf)\n",
    "\n",
    "# AUC (area under curve)\n",
    "roc_auc_manual = auc(fpr_manual, tpr_manual)\n",
    "roc_auc_sklearn = auc(fpr_sklearn, tpr_sklearn)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(fpr_manual, tpr_manual, label=f'Manual (AUC = {roc_auc_manual:.3f})')\n",
    "plt.plot(fpr_sklearn, tpr_sklearn, label=f'Scikit (AUC = {roc_auc_sklearn:.3f})')\n",
    "plt.plot([0,1], [0,1], 'k--')  # baseline\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Manual Logistic Regression\")\n",
    "print(\"Confusion Matrix:\\n\", cm_manual)\n",
    "print(f\"Accuracy: {acc_manual:.3f}\")\n",
    "print(f\"Precision: {prec_manual:.3f}\")\n",
    "print(f\"Recall: {recall_manual:.3f}\")\n",
    "\n",
    "print(\"\\nScikit-learn Logistic Regression\")\n",
    "print(\"Confusion Matrix:\\n\", cm_sklearn)\n",
    "print(f\"Accuracy: {acc_sklearn:.3f}\")\n",
    "print(f\"Precision: {prec_sklearn:.3f}\")\n",
    "print(f\"Recall: {recall_sklearn:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
